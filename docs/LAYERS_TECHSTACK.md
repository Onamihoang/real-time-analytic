# Chi Tiáº¿t CÃ¡c Layer vÃ  Tech Stack

## Má»¥c Lá»¥c
1. [Tá»•ng Quan Kiáº¿n TrÃºc](#tá»•ng-quan-kiáº¿n-trÃºc)
2. [Layer 1: Orchestration & Workflow](#layer-1-orchestration--workflow)
3. [Layer 2: Data Streaming](#layer-2-data-streaming)
4. [Layer 3: Analytics Engine](#layer-3-analytics-engine)
5. [Layer 4: Storage](#layer-4-storage)
6. [Layer 5: Visualization](#layer-5-visualization)
7. [Infrastructure & DevOps](#infrastructure--devops)
8. [Data Flow Chi Tiáº¿t](#data-flow-chi-tiáº¿t)
9. [Production Best Practices](#production-best-practices)

---

## Tá»•ng Quan Kiáº¿n TrÃºc

Há»‡ thá»‘ng sá»­ dá»¥ng **Lambda Architecture** káº¿t há»£p vá»›i **Event-Driven Architecture**:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    LAMBDA ARCHITECTURE                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”‚
â”‚  â”‚  Speed Layer â”‚         â”‚  Batch Layer â”‚                 â”‚
â”‚  â”‚   (Druid     â”‚         â”‚   (Future:   â”‚                 â”‚
â”‚  â”‚ Real-time    â”‚         â”‚    Spark)    â”‚                 â”‚
â”‚  â”‚  Ingestion)  â”‚         â”‚              â”‚                 â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚
â”‚         â”‚                        â”‚                          â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                          â”‚
â”‚                      â–¼                                       â”‚
â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                               â”‚
â”‚              â”‚ Serving Layerâ”‚                               â”‚
â”‚              â”‚    (Druid    â”‚                               â”‚
â”‚              â”‚    Query)    â”‚                               â”‚
â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Äáº·c Ä‘iá»ƒm:**
- âœ… Real-time ingestion vá»›i latency < 1s
- âœ… OLAP queries vá»›i sub-second response time
- âœ… Horizontal scalability
- âœ… Fault-tolerant vá»›i message replay tá»« Kafka
- âœ… Columnar storage vá»›i high compression ratio

---

## Layer 1: Orchestration & Workflow

### Apache Airflow 2.2.5

#### Vai TrÃ²
Airflow lÃ  "conductor" cá»§a toÃ n bá»™ data pipeline, chá»‹u trÃ¡ch nhiá»‡m:
- â° **Scheduling:** Trigger tasks theo schedule (cron expressions)
- ğŸ”„ **Workflow Management:** Quáº£n lÃ½ dependencies giá»¯a cÃ¡c tasks
- ğŸ“Š **Monitoring:** Track task status, retry failed tasks
- ğŸ”§ **Orchestration:** Coordinate giá»¯a nhiá»u systems

#### Components

##### 1. Airflow Scheduler
```python
# Pseudo-code cá»§a scheduler loop
while True:
    dags = load_dags_from("/airflow/dags")
    for dag in dags:
        if should_trigger(dag.schedule_interval):
            create_dag_run(dag)

    for task in get_scheduled_tasks():
        if dependencies_met(task):
            execute_task(task)

    sleep(scheduler_heartbeat)
```

**Configuration:**
- `executor`: SequentialExecutor (demo) / CeleryExecutor (production)
- `parallelism`: 32 tasks globally
- `dag_concurrency`: 16 tasks per DAG
- `scheduler_heartbeat_sec`: 5 seconds

##### 2. Airflow Webserver
- **Port:** 3000 (external), 8080 (internal)
- **Framework:** Flask + WTForms
- **Authentication:** Basic auth (demo) / LDAP, OAuth (production)
- **Features:**
  - DAG visualization (Graph View, Tree View, Gantt)
  - Task logs viewer
  - Variable & connection management
  - Manual trigger & backfill

##### 3. Metadata Database
- **Type:** SQLite (demo) â†’ PostgreSQL/MySQL (production)
- **Schema:**
  - `dag`: DAG definitions
  - `dag_run`: DAG execution instances
  - `task_instance`: Task execution records
  - `xcom`: Cross-communication data
  - `variable`: Airflow variables
  - `connection`: External system credentials

#### DAG Configuration (demo.py)

```python
# File: /app_airflow/app/dags/demo.py
from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime, timedelta

default_args = {
    'owner': 'airflow',
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

dag = DAG(
    dag_id='Demo',
    default_args=default_args,
    description='Produce crypto price data to Kafka',
    schedule_interval='*/1 * * * *',  # Every minute
    start_date=datetime(2023, 1, 1),
    catchup=False,  # Don't backfill
)

def demo_func():
    """Generate and send messages to Kafka"""
    from kafka import KafkaProducer
    import json, random, time

    producer = KafkaProducer(
        bootstrap_servers=['kafka:9092'],
        value_serializer=lambda v: json.dumps(v).encode('utf-8')
    )

    coins = {
        'BTC': (100, 200),
        'ETH': (80, 150),
        'DOT': (20, 50),
        'BTT': (1, 10),
    }

    for name, (min_val, max_val) in coins.items():
        message = {
            'data_id': random.randint(min_val, max_val),
            'name': name,
            'timestamp': int(time.time()),
        }
        producer.send('demo', value=message)

    producer.flush()
    producer.close()

task = PythonOperator(
    task_id='produce_crypto_data',
    python_callable=demo_func,
    dag=dag,
)
```

#### Tech Stack
| Component | Technology | Version | Purpose |
|-----------|-----------|---------|---------|
| Core | Apache Airflow | 2.2.5 | Workflow engine |
| Language | Python | 3.9 | DAG development |
| Executor | SequentialExecutor | - | Task execution (demo) |
| Metadata DB | SQLite | - | Airflow metadata (demo) |
| Webserver | Flask | - | Web UI |
| Scheduler | APScheduler | - | Cron-based scheduling |

#### Dockerfile
```dockerfile
FROM python:3.9-slim

# Install Airflow + dependencies
RUN pip install apache-airflow==2.2.5
RUN pip install kafka-python==2.0.2 psycopg2==2.9.3

# Copy DAGs and config
COPY app/ /airflow/

# Initialize Airflow DB
RUN airflow db init

# Start scheduler + webserver
CMD airflow scheduler & airflow webserver
```

#### Dependencies
```
# requirements.txt
apache-airflow==2.2.5
kafka-python==2.0.2    # Kafka producer client
psycopg2==2.9.3        # PostgreSQL driver (for production)
numpy==1.24.2          # Numerical computing
pandas==1.5.3          # Data manipulation
vnstock==0.1.4         # Vietnamese stock data (optional)
```

---

## Layer 2: Data Streaming

### Apache Kafka + Zookeeper

#### Apache Kafka 5.2.0 (Confluent Platform)

##### Vai TrÃ²
Kafka lÃ  "event bus" cá»§a há»‡ thá»‘ng:
- ğŸ“¨ **Message Broker:** LÆ°u trá»¯ vÃ  phÃ¢n phá»‘i messages
- ğŸ” **Decoupling:** TÃ¡ch biá»‡t producers vÃ  consumers
- ğŸ’¾ **Durability:** Persist messages vá»›i configurable retention
- ğŸ”„ **Replay:** Consumer cÃ³ thá»ƒ re-consume messages tá»« báº¥t ká»³ offset nÃ o

##### Architecture
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           Kafka Broker                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Topic: "demo"                          â”‚
â”‚  â”œâ”€ Partition 0 (Leader)                â”‚
â”‚  â”‚  â”œâ”€ Segment 0: [msg0...msg999]      â”‚
â”‚  â”‚  â”œâ”€ Segment 1: [msg1000...msg1999]  â”‚
â”‚  â”‚  â””â”€ ...                              â”‚
â”‚  â””â”€ (No replicas - replication=1)       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

##### Configuration (docker-compose.yaml)
```yaml
kafka:
  image: confluentinc/cp-kafka:latest
  environment:
    KAFKA_BROKER_ID: 1
    KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181

    # Listeners
    KAFKA_ADVERTISED_LISTENERS: |
      PLAINTEXT://kafka:9092,
      PLAINTEXT_HOST://localhost:29092
    KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: |
      PLAINTEXT:PLAINTEXT,
      PLAINTEXT_HOST:PLAINTEXT

    # Topic settings
    KAFKA_AUTO_CREATE_TOPICS_ENABLE: "true"
    KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
    KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
    KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1

  ports:
    - "29092:29092"  # External access

  depends_on:
    - zookeeper
```

##### Topic Configuration
- **Topic Name:** `demo`
- **Partitions:** 1 (single partition - demo)
- **Replication Factor:** 1 (no replication - demo)
- **Retention:** Default 7 days
- **Compression:** None (default)
- **Message Format:** JSON UTF-8

##### Message Schema
```json
{
  "data_id": 150,        // Integer: Random price value
  "name": "BTC",         // String: Cryptocurrency symbol
  "timestamp": 1645270401 // Integer: Unix timestamp (seconds)
}
```

#### Apache Zookeeper

##### Vai TrÃ²
Zookeeper lÃ  "configuration manager":
- ğŸ—‚ï¸ **Metadata Storage:** LÆ°u cluster configuration
- ğŸ‘‘ **Leader Election:** Chá»n partition leaders
- ğŸ”” **Notification:** ThÃ´ng bÃ¡o thay Ä‘á»•i cluster state
- ğŸ”’ **Distributed Lock:** Coordination giá»¯a brokers

##### Configuration
```yaml
zookeeper:
  image: confluentinc/cp-zookeeper:latest
  environment:
    ZOOKEEPER_CLIENT_PORT: 2181
    ZOOKEEPER_TICK_TIME: 2000  # Heartbeat interval
  ports:
    - "22181:2181"
```

##### ZooKeeper Data Structure
```
/
â”œâ”€â”€ brokers/
â”‚   â”œâ”€â”€ ids/          # Broker registrations
â”‚   â”œâ”€â”€ topics/       # Topic metadata
â”‚   â””â”€â”€ seqid/        # Sequence IDs
â”œâ”€â”€ consumers/        # Consumer group info
â”œâ”€â”€ config/           # Configurations
â””â”€â”€ controller        # Current controller broker
```

#### Kafka Producer (trong DAG)

```python
from kafka import KafkaProducer
import json

producer = KafkaProducer(
    bootstrap_servers=['kafka:9092'],

    # Serialization
    value_serializer=lambda v: json.dumps(v).encode('utf-8'),

    # Performance tuning
    acks=1,              # Wait for leader ACK (not all replicas)
    retries=3,           # Retry failed sends
    batch_size=16384,    # Batch size in bytes
    linger_ms=10,        # Wait 10ms before sending batch

    # Compression (optional)
    compression_type='none',  # or 'gzip', 'snappy', 'lz4'
)

# Send message
future = producer.send('demo', value=message)

# Wait for confirmation (blocking)
metadata = future.get(timeout=10)
print(f"Sent to partition {metadata.partition} at offset {metadata.offset}")

# Cleanup
producer.flush()
producer.close()
```

#### Tech Stack
| Component | Technology | Version | Purpose |
|-----------|-----------|---------|---------|
| Message Broker | Apache Kafka | 5.2.0 | Event streaming |
| Coordination | Apache Zookeeper | Latest | Cluster management |
| Producer Client | kafka-python | 2.0.2 | Python Kafka client |
| Serialization | JSON | - | Message format |

---

## Layer 3: Analytics Engine

### Apache Druid 0.22.1+

#### Vai TrÃ²
Druid lÃ  "analytical database":
- âš¡ **Fast Queries:** Sub-second aggregations on billions of rows
- ğŸ“Š **OLAP Workloads:** Multi-dimensional analytics
- â±ï¸ **Time-Series Optimized:** Efficient time-based queries
- ğŸ”¥ **Real-time Ingestion:** Stream processing vá»›i low latency
- ğŸ“¦ **Columnar Storage:** High compression & scan performance

#### Architecture Components

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   Apache Druid Cluster                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚  â”‚    Router    â”‚â”€â”€â”€â”€â”€â”€â”€â–¶â”‚    Broker    â”‚              â”‚
â”‚  â”‚    :8888     â”‚        â”‚    :8082     â”‚              â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜              â”‚
â”‚                                   â”‚                      â”‚
â”‚                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚                        â”‚                     â”‚          â”‚
â”‚                        â–¼                     â–¼          â”‚
â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚              â”‚  Historical  â”‚     â”‚ MiddleManagerâ”‚     â”‚
â”‚              â”‚    :8083     â”‚     â”‚    :8091     â”‚     â”‚
â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚                        â”‚                     â”‚          â”‚
â”‚                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â”‚                                   â–¼                      â”‚
â”‚                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚
â”‚                        â”‚   Coordinator    â”‚             â”‚
â”‚                        â”‚      :8081       â”‚             â”‚
â”‚                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚
â”‚                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚                              â”‚
            â–¼                              â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  PostgreSQL  â”‚            â”‚ Zookeeper    â”‚
    â”‚   Metadata   â”‚            â”‚   Cluster    â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚     State    â”‚
                                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### 1. Druid Router (:8888)

**Vai trÃ²:** Unified API endpoint

**Responsibilities:**
- Route queries Ä‘áº¿n Broker nodes
- Route management requests Ä‘áº¿n Coordinator
- Load balancing across Brokers
- Authentication & authorization (náº¿u enable)

**Endpoints:**
- `GET /status` - Health check
- `POST /druid/v2/sql/` - SQL queries
- `POST /druid/v2/` - Native queries
- `GET /druid/coordinator/v1/` - Coordinator management API

**Configuration:**
```yaml
druid_router:
  environment:
    - druid_host=router
    - druid_service=druid/router
    - druid_plaintextPort=8888

    # Routing rules
    - druid_router_defaultBrokerServiceName=druid/broker
    - druid_router_coordinatorServiceName=druid/coordinator

    # Connection pooling
    - druid_router_http_numConnections=50
    - druid_router_http_readTimeout=PT5M
```

#### 2. Druid Broker (:8082)

**Vai trÃ²:** Query execution engine

**Responsibilities:**
- Accept queries tá»« clients
- Fetch segment metadata tá»« PostgreSQL
- Distribute sub-queries Ä‘áº¿n Historical/MiddleManager
- Merge results tá»« data nodes
- Apply LIMIT, ORDER BY, final aggregations

**Query Processing:**
```
1. Parse SQL â†’ Native Druid Query
   SELECT name, AVG(data_id) FROM demo GROUP BY name
   â†“
   {
     "queryType": "groupBy",
     "dataSource": "demo",
     "dimensions": ["name"],
     "aggregations": [{"type":"doubleSum", "name":"sum_data_id", ...}],
     ...
   }

2. Fetch Segment Metadata
   â†“ Query PostgreSQL
   Segments: [
     {id: "demo_2024-01-01T00:00:00.000Z_...", size: 1MB, location: "historical:8083"},
     {id: "demo_2024-01-01T01:00:00.000Z_...", size: 500KB, location: "middlemanager:8100"},
   ]

3. Prune Segments by Time Range
   â†“ Filter segments matching WHERE clause time range

4. Scatter Queries to Data Nodes
   â†“ Send sub-queries in parallel
   â†’ Historical: query segments [seg1, seg2, seg3]
   â†’ MiddleManager: query real-time segment [seg4]

5. Gather Partial Results
   â† Historical: {BTC: sum=1500, count=10}
   â† MiddleManager: {BTC: sum=300, count=2}

6. Merge & Finalize
   â†“ Combine: {BTC: avg = (1500+300)/(10+2) = 150}

7. Return to Client
   â†’ [{name: "BTC", avg_data_id: 150}, ...]
```

**Configuration:**
```properties
# Processing
druid.processing.buffer.sizeBytes=134217728  # 128MB
druid.processing.numThreads=2
druid.processing.numMergeBuffers=2

# Caching
druid.broker.cache.useCache=true
druid.broker.cache.populateCache=true
druid.cache.type=caffeine
druid.cache.sizeInBytes=268435456  # 256MB
```

#### 3. Druid Coordinator (:8081)

**Vai trÃ²:** Cluster management

**Responsibilities:**
- Monitor segment availability
- Assign segments Ä‘áº¿n Historical nodes
- Load balancing segments across cluster
- Drop old segments theo retention rules
- Compact small segments
- Manage replication

**Load Rules:**
```json
[
  {
    "type": "loadForever",  // Keep all data
    "tieredReplicants": {
      "_default_tier": 1    // 1 replica (no replication in demo)
    }
  }
]
```

**UI:** http://localhost:8081 - Cluster overview, segment browser

#### 4. Druid Historical (:8083)

**Vai trÃ²:** Long-term segment storage & queries

**Responsibilities:**
- Load segments tá»« deep storage (/opt/shared)
- Cache segments in memory/disk
- Serve queries cho immutable segments
- Announce segments Ä‘áº¿n Zookeeper

**Segment Structure:**
```
/opt/shared/segments/demo/
â””â”€â”€ 2024-01-01T00:00:00.000Z_2024-01-01T01:00:00.000Z/
    â””â”€â”€ 2024-01-01T00:05:00.123Z/
        â”œâ”€â”€ 0/
        â”‚   â”œâ”€â”€ index.zip          # Compressed columnar data
        â”‚   â”‚   â”œâ”€â”€ __time.column  # Timestamp column
        â”‚   â”‚   â”œâ”€â”€ name.column    # String column (dictionary encoded)
        â”‚   â”‚   â”œâ”€â”€ data_id.column # Numeric column
        â”‚   â”‚   â””â”€â”€ metadata.json  # Segment metadata
        â”‚   â””â”€â”€ descriptor.json    # Segment descriptor
        â””â”€â”€ version.bin
```

**Segment Format (Columnar):**
```
Row-based (traditional):
[{time:T1, name:BTC, id:150}, {time:T2, name:ETH, id:100}, ...]

Column-based (Druid):
__time:   [T1, T2, T3, T4, ...]
name:     [BTC, ETH, BTC, DOT, ...]  â†’ Dictionary: {0:BTC, 1:ETH, 2:DOT} â†’ [0,1,0,2,...]
data_id:  [150, 100, 155, 30, ...]
```

**Advantages:**
- âœ… High compression (dictionary encoding, run-length encoding)
- âœ… Fast scans (only read needed columns)
- âœ… Efficient aggregations (vectorized operations)

**Configuration:**
```properties
# Segment cache
druid.segmentCache.locations=[{"path":"/opt/druid/var/druid/segment-cache","maxSize":10g}]

# Processing threads
druid.processing.numThreads=2
druid.processing.buffer.sizeBytes=134217728
```

#### 5. Druid MiddleManager (:8091, :8100-8105)

**Vai trÃ²:** Real-time ingestion & indexing

**Responsibilities:**
- Consume tá»« Kafka topic "demo"
- Build real-time segments in memory
- Persist segments Ä‘áº¿n deep storage
- Hand-off segments cho Historical
- Execute indexing tasks (batch ingestion)

**Ingestion Spec (Kafka Supervisor):**
```json
{
  "type": "kafka",
  "dataSchema": {
    "dataSource": "demo",
    "timestampSpec": {
      "column": "timestamp",
      "format": "posix"
    },
    "dimensionsSpec": {
      "dimensions": ["name"]
    },
    "metricsSpec": [
      {"type": "count", "name": "count"},
      {"type": "longSum", "name": "sum_data_id", "fieldName": "data_id"}
    ],
    "granularitySpec": {
      "type": "uniform",
      "segmentGranularity": "HOUR",
      "queryGranularity": "MINUTE"
    }
  },
  "tuningConfig": {
    "type": "kafka",
    "maxRowsInMemory": 100000,
    "maxBytesInMemory": 134217728,
    "intermediatePersistPeriod": "PT10M",
    "maxPendingPersists": 0
  },
  "ioConfig": {
    "topic": "demo",
    "consumerProperties": {
      "bootstrap.servers": "kafka:9092"
    },
    "taskCount": 1,
    "replicas": 1,
    "taskDuration": "PT1H"
  }
}
```

**Indexing Flow:**
```
1. Consume from Kafka
   â†“ Poll messages from topic "demo"

2. Parse & Validate
   â†“ Extract timestamp, dimensions, metrics

3. Build In-Memory Index
   â†“ Accumulate rows in heap
   â†“ When maxRowsInMemory reached â†’ persist to disk

4. Create Segment
   â†“ After taskDuration (1 hour) or manual trigger
   â†“ Merge all persisted files
   â†“ Build final columnar segment

5. Publish to Deep Storage
   â†“ Upload segment to /opt/shared/

6. Notify Coordinator
   â†“ Register segment metadata in PostgreSQL

7. Hand-off to Historical
   â†“ Coordinator assigns segment to Historical
   â†“ Historical loads segment
   â†“ MiddleManager drops in-memory segment
```

#### Druid Extensions (app_druid/environment.env)

```properties
druid_extensions_loadList=[
  "druid-kafka-indexing-service",    # Kafka ingestion
  "druid-histogram",                  # Histogram aggregations
  "druid-datasketches",               # Approximate algorithms (HLL, Theta)
  "druid-multi-stage-query",          # Complex query engine
  "postgresql-metadata-storage"       # PostgreSQL metadata
]
```

#### Tech Stack
| Component | Technology | Version | Purpose |
|-----------|-----------|---------|---------|
| Analytics DB | Apache Druid | 0.22.1+ | OLAP engine |
| Metadata Storage | PostgreSQL | 14.1 | Segment metadata |
| Coordination | Zookeeper | Latest | Cluster state |
| Deep Storage | Local FS | - | Segment storage |
| Query Language | SQL + Native | - | Query interface |
| Ingestion | Kafka Indexing Service | - | Stream ingestion |

---

## Layer 4: Storage

### PostgreSQL 14.1 (Metadata Storage)

**Vai trÃ²:** Metadata repository cho Druid

**LÆ°u trá»¯:**
- âœ… Segment metadata (location, size, time range)
- âœ… Supervisor & task status
- âœ… Load rules & compaction configs
- âœ… Audit logs

**Schema:**
```sql
-- Segments table
CREATE TABLE druid_segments (
    id VARCHAR(255) PRIMARY KEY,
    dataSource VARCHAR(255),
    created_date VARCHAR(255),
    start VARCHAR(255),
    "end" VARCHAR(255),
    partitioned BOOLEAN,
    version VARCHAR(255),
    used BOOLEAN,
    payload BYTEA
);

-- Supervisors table
CREATE TABLE druid_supervisors (
    id VARCHAR(255) PRIMARY KEY,
    spec_id VARCHAR(255),
    created_date VARCHAR(255),
    payload BYTEA
);
```

**Configuration:**
```yaml
postgres:
  image: postgres:14.1-alpine
  environment:
    POSTGRES_PASSWORD: FoolishPassword
    POSTGRES_USER: druid
    POSTGRES_DB: druid
  ports:
    - "5432:5432"
```

**Connection tá»« Druid:**
```properties
druid.metadata.storage.type=postgresql
druid.metadata.storage.connector.connectURI=jdbc:postgresql://postgres:5432/druid
druid.metadata.storage.connector.user=druid
druid.metadata.storage.connector.password=FoolishPassword
```

### Local File System (Deep Storage)

**Path:** `/opt/shared/segments/`

**Structure:**
```
/opt/shared/
â”œâ”€â”€ segments/
â”‚   â””â”€â”€ demo/                                    # Datasource
â”‚       â”œâ”€â”€ 2024-01-01T00:00:00.000Z_..._v1/    # Segment
â”‚       â”‚   â””â”€â”€ 0/
â”‚       â”‚       â””â”€â”€ index.zip
â”‚       â””â”€â”€ 2024-01-01T01:00:00.000Z_..._v1/
â”‚           â””â”€â”€ 0/
â”‚               â””â”€â”€ index.zip
â””â”€â”€ task/                                        # Task working directory
    â””â”€â”€ [temp files]
```

**Volume Mounting:**
```yaml
volumes:
  - druid_shared:/opt/shared

volumes:
  druid_shared:
```

**Production Alternatives:**
- â˜ï¸ **AWS S3:** `druid.storage.type=s3`, `druid.storage.bucket=my-bucket`
- â˜ï¸ **Google Cloud Storage:** `druid.storage.type=google`
- â˜ï¸ **Azure Blob:** `druid.storage.type=azure`
- ğŸ—„ï¸ **HDFS:** `druid.storage.type=hdfs`

### Redis (Optional Caching)

**Vai trÃ²:** Session storage, caching

**Use cases:**
- Airflow session management
- Superset query result caching
- Temporary data storage

**Configuration:**
```yaml
redis:
  image: redis:latest
  ports:
    - "6379:6379"
```

---

## Layer 5: Visualization

### Apache Superset 1.4.1

**Vai trÃ²:** Business Intelligence platform

**Features:**
- ğŸ“Š **Charts:** 50+ visualization types (line, bar, pie, heatmap, etc.)
- ğŸ“ˆ **Dashboards:** Drag-and-drop dashboard builder
- ğŸ” **SQL Lab:** Ad-hoc SQL queries vá»›i autocomplete
- ğŸ” **Access Control:** Role-based permissions
- ğŸ“… **Scheduled Reports:** Email reports theo schedule
- ğŸ¨ **Themes:** Customizable UI themes

**Druid Connection:**
```python
# SQLAlchemy URI
SQLALCHEMY_DATABASE_URI = 'druid://broker:8082/druid/v2/sql/'

# Example query from Superset
query = """
SELECT
    name,
    TIME_FLOOR(__time, 'PT1H') as hour,
    AVG(data_id) as avg_price,
    MAX(data_id) as max_price,
    MIN(data_id) as min_price,
    COUNT(*) as count
FROM demo
WHERE __time >= CURRENT_TIMESTAMP - INTERVAL '24' HOUR
GROUP BY name, TIME_FLOOR(__time, 'PT1H')
ORDER BY hour DESC
"""
```

**Chart Types Examples:**

1. **Time Series Line Chart:**
   - X-axis: `__time` (time column)
   - Y-axis: `AVG(data_id)`
   - Group by: `name` (BTC, ETH, DOT, BTT)

2. **Bar Chart:**
   - Dimension: `name`
   - Metric: `SUM(data_id)`

3. **Pie Chart:**
   - Dimension: `name`
   - Metric: `COUNT(*)`

**Configuration:**
```yaml
superset:
  image: amancevice/superset:1.4.1
  environment:
    - SUPERSET_SECRET_KEY=your_secret_key_here
  ports:
    - "8088:8088"
  command: |
    superset db upgrade &&
    superset fab create-admin \
      --username admin \
      --password admin \
      --firstname Admin \
      --lastname User \
      --email admin@superset.com &&
    superset init &&
    superset run -h 0.0.0.0 -p 8088
```

**Access:**
- URL: http://localhost:8088
- Default credentials: admin / admin

---

## Infrastructure & DevOps

### Docker & Docker Compose

**Version:** Docker Compose 3.8

**Services:**
- âœ… 10 containers in total
- âœ… Networked via bridge network
- âœ… Persistent volumes for data
- âœ… Health checks (optional)
- âœ… Dependency management

**docker-compose.yaml Structure:**
```yaml
version: '3.8'

services:
  zookeeper: ...
  postgres: ...
  kafka: ...
  druid_coordinator: ...
  druid_broker: ...
  druid_historical: ...
  druid_middlemanager: ...
  druid_router: ...
  airflow: ...
  superset: ...

volumes:
  druid_shared:
  postgres_data:

networks:
  default:
    driver: bridge
```

**Startup Command:**
```bash
# Start all services
docker-compose up -d

# Check status
docker-compose ps

# View logs
docker-compose logs -f [service]

# Stop all
docker-compose down

# Remove volumes
docker-compose down -v
```

---

## Data Flow Chi Tiáº¿t

### End-to-End Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Clock     â”‚ Every minute (cron: */1 * * * *)
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 1: Airflow Triggers DAG                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Airflow Scheduler â†’ Create DagRun instance             â”‚
â”‚                   â†’ Execute demo_func()                 â”‚
â”‚                   â†’ Generate 4 messages                 â”‚
â”‚ Time: ~1-2 seconds                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 2: Produce to Kafka                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ KafkaProducer â†’ Serialize JSON to bytes                â”‚
â”‚               â†’ Send to topic "demo"                    â”‚
â”‚               â†’ Kafka appends to partition log          â”‚
â”‚ Latency: <50ms                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 3: Druid Ingestion (Real-time)                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ MiddleManager â†’ Poll Kafka (fetch.min.bytes=1)         â”‚
â”‚               â†’ Parse JSON & extract fields             â”‚
â”‚               â†’ Add to in-memory segment                â”‚
â”‚               â†’ Build columnar index                    â”‚
â”‚ Latency: <500ms (queryable immediately)                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 4: Segment Handoff (Every hour)                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ MiddleManager â†’ Finalize segment                       â”‚
â”‚               â†’ Persist to /opt/shared/                 â”‚
â”‚               â†’ Update PostgreSQL metadata              â”‚
â”‚ Coordinator   â†’ Assign segment to Historical           â”‚
â”‚ Historical    â†’ Load segment from deep storage         â”‚
â”‚ MiddleManager â†’ Drop in-memory segment                 â”‚
â”‚ Time: ~10-30 seconds                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 5: Query Execution                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ User â†’ Superset â†’ Router â†’ Broker                      â”‚
â”‚ Broker â†’ Fetch segment metadata from PostgreSQL        â”‚
â”‚        â†’ Query Historical (old segments)                â”‚
â”‚        â†’ Query MiddleManager (recent data)              â”‚
â”‚        â†’ Merge results                                  â”‚
â”‚ Router â†’ Return to Superset                             â”‚
â”‚ Superset â†’ Render chart                                 â”‚
â”‚ Latency: 50ms-2s depending on query complexity          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Data Latency Breakdown

| Stage | Latency | Description |
|-------|---------|-------------|
| Airflow schedule trigger | 0-60s | Cron precision (1 minute granularity) |
| Message generation | 1-2s | Python execution time |
| Kafka produce | <50ms | Network + broker write |
| Druid consume | <500ms | Poll interval + parsing |
| **Query latency (real-time)** | **<1s** | **End-to-end from produce to queryable** |
| Segment handoff | 1-60min | Based on taskDuration config |
| Query execution (simple) | <100ms | Aggregation on indexed data |
| Query execution (complex) | 100ms-2s | Multi-table joins, large scans |

---

## Production Best Practices

### 1. Scalability

**Airflow:**
- âŒ SequentialExecutor (demo) â†’ âœ… CeleryExecutor / KubernetesExecutor
- âŒ SQLite metadata â†’ âœ… PostgreSQL / MySQL
- âœ… Add Celery workers Ä‘á»ƒ parallel task execution
- âœ… Redis/RabbitMQ lÃ m message broker cho Celery

**Kafka:**
- âŒ Single broker â†’ âœ… Cluster with 3+ brokers
- âŒ Replication factor = 1 â†’ âœ… Replication = 3
- âœ… Multiple partitions cho high throughput topics
- âœ… Enable compression (lz4, snappy)

**Druid:**
- âœ… Multiple Historical nodes (scale horizontally)
- âœ… Multiple MiddleManager nodes
- âœ… Multiple Broker nodes behind load balancer
- âŒ Local storage â†’ âœ… S3/GCS/HDFS
- âœ… Tiered storage (hot/cold data)

### 2. High Availability

**Infrastructure:**
- âœ… Zookeeper ensemble (3 hoáº·c 5 nodes)
- âœ… PostgreSQL replication (master-slave)
- âœ… Kafka replication across availability zones
- âœ… Load balancers cho Druid Router & Broker

**Monitoring:**
- âœ… Prometheus + Grafana
- âœ… ELK stack (Elasticsearch, Logstash, Kibana)
- âœ… Alerting (PagerDuty, Slack)

### 3. Security

**Authentication:**
- âœ… Airflow: LDAP / OAuth2
- âœ… Superset: LDAP / SAML
- âœ… Kafka: SASL/SCRAM or SASL/PLAIN
- âœ… Druid: Basic Auth / Kerberos

**Encryption:**
- âœ… SSL/TLS cho táº¥t cáº£ connections
- âœ… Encrypt data at rest (disk encryption)
- âœ… Kafka SSL encryption

**Network:**
- âœ… VPC / Private subnets
- âœ… Security groups / Firewall rules
- âœ… Bastion hosts cho admin access

### 4. Performance Tuning

**Druid:**
```properties
# Increase processing threads
druid.processing.numThreads=8  # = CPU cores - 1

# Increase buffer size
druid.processing.buffer.sizeBytes=536870912  # 512MB

# Enable caching
druid.broker.cache.sizeInBytes=2147483648  # 2GB
druid.historical.cache.sizeInBytes=2147483648

# Query optimization
druid.query.groupBy.maxMergingDictionarySize=100000000
druid.query.groupBy.maxOnDiskStorage=10737418240  # 10GB
```

**Kafka:**
```properties
# Producer
batch.size=32768
linger.ms=10
compression.type=lz4
acks=all

# Consumer
fetch.min.bytes=1048576  # 1MB
max.poll.records=500
```

### 5. Monitoring Metrics

**Airflow:**
- âœ… DAG run duration
- âœ… Task success/failure rate
- âœ… Scheduler lag
- âœ… Executor queue size

**Kafka:**
- âœ… Broker CPU/Memory/Disk usage
- âœ… Partition lag
- âœ… Producer/Consumer throughput
- âœ… Under-replicated partitions

**Druid:**
- âœ… Query latency (p50, p95, p99)
- âœ… Segment count & size
- âœ… Ingestion rate
- âœ… Query cache hit rate
- âœ… JVM heap usage

---

## Káº¿t Luáº­n

Há»‡ thá»‘ng Real-Time Analytics nÃ y lÃ  má»™t vÃ­ dá»¥ hoÃ n chá»‰nh vá»:

âœ… **Lambda Architecture** - Káº¿t há»£p speed layer (real-time) vÃ  batch layer (historical)
âœ… **Event-Driven Architecture** - Kafka lÃ m event bus
âœ… **Microservices** - Má»—i component Ä‘á»™c láº­p, scale riÃªng
âœ… **OLAP Analytics** - Druid optimized cho analytical queries
âœ… **Workflow Orchestration** - Airflow quáº£n lÃ½ dependencies
âœ… **Data Visualization** - Superset cho dashboards

**Äiá»ƒm máº¡nh:**
- âš¡ Low latency (<1s tá»« ingestion Ä‘áº¿n queryable)
- ğŸ“ˆ Horizontal scalability
- ğŸ”„ Fault-tolerant vá»›i Kafka replay
- ğŸ’¾ Efficient columnar storage
- ğŸ¯ Production-ready architecture (vá»›i modifications)

**Production Checklist:**
- [ ] Migrate to CeleryExecutor (Airflow)
- [ ] Kafka cluster vá»›i replication â‰¥ 3
- [ ] PostgreSQL replication
- [ ] S3/GCS deep storage (Druid)
- [ ] Multiple Druid nodes
- [ ] SSL/TLS encryption
- [ ] Authentication & authorization
- [ ] Monitoring stack (Prometheus/Grafana)
- [ ] Backup & disaster recovery plan
- [ ] Auto-scaling (Kubernetes/ECS)
